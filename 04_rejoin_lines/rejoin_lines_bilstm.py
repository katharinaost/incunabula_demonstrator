# largely generated by ChatGPT 5

import argparse
import json
import re
import sys
from pathlib import Path
from typing import List, Tuple

import torch
import torch.nn as nn

# -----------------------
# Text normalization / utils
# -----------------------

SOFT_HYPHEN = "\u00ad"
HYPHENS = "-\u2010\u2011\u2012\u2013\u2014\u2212"  # -, ‐, -, ‒, –, —, −
BOUNDARY_TOKEN = "|"  # must exist in the vocab used during training

left_n = 12                 # chars taken from end of first line
right_n = 12                # chars taken from start of second line

def normalize(s: str) -> str:
    if s is None:
        return ""
    s = str(s)
    s = s.replace(SOFT_HYPHEN, "")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def strip_trailing_hyphen(s: str) -> str:
    # remove exactly one trailing hyphen-like char if present
    return re.sub(f"[{re.escape(HYPHENS)}]$", "", s)

# -----------------------
# Model definition (must match training architecture)
# -----------------------

PAD = 0
UNK = 1

class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, hidden_size: int,
                 num_layers: int = 1, bidirectional: bool = True, dropout: float = 0.0):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD)
        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers,
                            batch_first=True, bidirectional=bidirectional)
        self.dropout = nn.Dropout(dropout)
        out_dim = hidden_size * (2 if bidirectional else 1)
        self.fc = nn.Linear(out_dim, 1)

    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:
        # x: [B, T], lengths: [B]
        emb = self.emb(x)
        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)
        _, (h_n, _) = self.lstm(packed)
        if self.lstm.bidirectional:
            h = torch.cat([h_n[-2], h_n[-1]], dim=1)  # last layer fwd/bwd
        else:
            h = h_n[-1]
        h = self.dropout(h)
        logits = self.fc(h).squeeze(1)
        return logits  # raw scores

# -----------------------
# Loading model + vocab (sizes inferred from state_dict)
# -----------------------

def infer_arch_from_state_dict(sd: dict) -> Tuple[int, int, int, bool]:
    """
    Returns (vocab_size, embed_dim, hidden_size, bidirectional).
    """
    vocab_size, embed_dim = sd["emb.weight"].shape
    hidden_size = sd["lstm.weight_ih_l0"].shape[0] // 4
    bidirectional = any(k.startswith("lstm.weight_ih_l0_reverse") for k in sd.keys())
    layer_indices = []
    for k in sd.keys():
        m = re.search(r"(?:^|\.)(?:lstm)\.weight_ih_l(\d+)(?:_reverse)?$", k)
        if m:
            layer_indices.append(int(m.group(1)))
    num_layers = (max(layer_indices) + 1) if layer_indices else 1
    return vocab_size, embed_dim, hidden_size, bidirectional, num_layers

def load_model(model_path: Path, device: torch.device) -> Tuple[BiLSTMClassifier, dict]:
    sd = torch.load(model_path, map_location=device)
    if not isinstance(sd, dict):
        raise ValueError("Model file does not contain a state_dict")
    vocab_size, embed_dim, hidden_size, bidirectional, num_layers = infer_arch_from_state_dict(sd)
    model = BiLSTMClassifier(
        vocab_size=vocab_size,
        embed_dim=embed_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        bidirectional=bidirectional,
        dropout=0.0,  # dropout irrelevant at inference
    ).to(device)
    model.load_state_dict(sd)
    model.eval()
    return model

def load_vocab(vocab_path: Path) -> dict:
    with open(vocab_path, "r", encoding="utf-8") as f:
        ch2idx = json.load(f)
    if BOUNDARY_TOKEN not in ch2idx:
        # Warn but continue (it should exist if you trained with it)
        print(f"WARNING: boundary token '{BOUNDARY_TOKEN}' not found in vocab.", file=sys.stderr)
    return ch2idx

# -----------------------
# Encoding windows + batch prediction
# -----------------------

def make_window(a: str, b: str, left_n: int, right_n: int) -> str:
    a = normalize(a)
    b = normalize(b)
    return f"{a[-left_n:]}{BOUNDARY_TOKEN}{b[:right_n]}"

def encode(seq: str, ch2idx: dict, max_len: int) -> List[int]:
    ids = [ch2idx.get(ch, UNK) for ch in seq]
    if len(ids) < max_len:
        ids = ids + [PAD] * (max_len - len(ids))
    else:
        ids = ids[:max_len]
    return ids

@torch.no_grad()
def predict_windows(model: BiLSTMClassifier, ch2idx: dict, windows: List[str], device: torch.device) -> List[float]:
    if not windows:
        return []
    max_len = max(len(w) for w in windows)
    X = torch.tensor([encode(w, ch2idx, max_len) for w in windows], dtype=torch.long, device=device)
    lengths = (X != PAD).sum(dim=1).to(torch.long)
    logits = model(X, lengths)
    probs = torch.sigmoid(logits).detach().cpu().tolist()
    return probs

def process_text(lines, model: BiLSTMClassifier, ch2idx: dict,
                          device: torch.device, left_n: int, right_n: int) -> str:
    lines = lines.splitlines()

    # Prepare windows for all boundaries in the paragraph
    windows = [make_window(lines[i], lines[i+1], left_n, right_n) for i in range(len(lines)-1)]
    print(windows)
    probs = predict_windows(model, ch2idx, windows, device)
    out_parts: List[str] = []
    for i, p in enumerate(probs):
        a = normalize(lines[i])
        b = normalize(lines[i+1])
        if p >= 0.5:
            a = strip_trailing_hyphen(a)
            out_parts.append(a)
        else:
            out_parts.append(a + " ")
    out_parts.append(normalize(lines[-1]))
    return "".join(out_parts)

# -----------------------
# CLI
# -----------------------

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load model + vocab
    try:
        model = load_model(args.model, device)
    except Exception as e:
        print(f"ERROR loading model from {args.model}: {e}", file=sys.stderr)
        sys.exit(2)
    try:
        ch2idx = load_vocab(args.vocab)
    except Exception as e:
        print(f"ERROR loading vocab from {args.vocab}: {e}", file=sys.stderr)
        sys.exit(2)

    if args.output:
        args.output.mkdir(parents=True, exist_ok=True)

    file = args.input
    try:
        text = file.read_text(encoding="utf-8")
    except Exception as e:
        print(f"ERROR reading {file}: {e}", file=sys.stderr)
        sys.exit(2)
        
    out_text = process_text(text, model, ch2idx, device, left_n, right_n)

    rel = file.name
    out_path = args.output / (Path(rel).stem + ".rejoin.txt")
    try:
        out_path.write_text(out_text, encoding="utf-8")
        print(f"Wrote {out_path}")
    except Exception as e:
        print(f"ERROR writing {out_path}: {e}", file=sys.stderr)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Reconstruct word breaks across line ends using a trained BiLSTM.")
    parser.add_argument("input", type=Path, help="Path to input file.")
    parser.add_argument("--output", type=Path, default=Path("output"), help="Output directory.")
    parser.add_argument("--model", type=Path, default=Path("bilstm_rejoin_model.pt"), help="Path to model state_dict.")
    parser.add_argument("--vocab", type=Path, default=Path("char_vocab.json"), help="Path to char vocab JSON.")
    args = parser.parse_args()
    main(args)
