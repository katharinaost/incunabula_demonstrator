# largely generated by ChatGPT 5

import argparse
import json, math
import numpy as np
import pandas as pd
from typing import List, Tuple
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report

# -----------------------
# Config
# -----------------------
left_n = 12                 # chars taken from end of first line
right_n = 12                # chars taken from start of second line
batch_size = 32
embed_dim = 64
hidden_size = 128
num_layers = 1
dropout = 0.5               # aggressive dropout helps a lot
learning_rate = 5e-3
epochs = 30
patience = 5                # early stopping
boundary_token = "|"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def make_window(a, b):
    left = a[-left_n:]
    right = b[:right_n]
    return f"{left}{boundary_token}{right}"

def load_data(path):
    df = pd.read_csv(path, sep="\t")
    df = df.dropna(subset=['word_break'])
    df = df.rename(columns={"last_15": "first", "first_15": "second", "word_break": "label"})
    df["label"] = df["label"].astype(int)
    df["window"] = [make_window(a,b) for a,b in zip(df["first"], df["second"])]
    return df[["window","label","first","second"]]

# -----------------------
# Vocabulary
# -----------------------
PAD = 0
UNK = 1

def build_vocab(seqs: List[str]) -> Tuple[dict, dict]:
    chars = set()
    for s in seqs:
        chars.update(list(s))
    # ensure boundary token is present
    chars.add(boundary_token)
    # common whitespace/punct likely to appear
    base = [boundary_token, " ", "-", "'", "Â·"]
    ordered = sorted(chars - set(base))      # stable ordering
    idx2ch = {PAD: "<PAD>", UNK: "<UNK>"}
    for i, ch in enumerate(base + ordered, start=2):
        idx2ch[i] = ch
    ch2idx = {ch: idx for idx, ch in idx2ch.items()}
    return ch2idx, idx2ch

def encode(seq: str, ch2idx: dict, max_len: int) -> List[int]:
    ids = [ch2idx.get(ch, UNK) for ch in seq]
    if len(ids) < max_len:
        ids = ids + [PAD] * (max_len - len(ids))
    else:
        ids = ids[:max_len]
    return ids

# -----------------------
# Dataset
# -----------------------
class BoundaryDataset(Dataset):
    def __init__(self, windows: List[str], labels: List[int], ch2idx: dict, max_len: int):
        self.windows = windows
        self.labels = labels
        self.ch2idx = ch2idx
        self.max_len = max_len
        
    def __len__(self):
        return len(self.windows)
    
    def __getitem__(self, i):
        seq = self.windows[i]
        x = torch.tensor(encode(seq, self.ch2idx, self.max_len), dtype=torch.long)
        # true length = count of non-PADs
        length = int((x != PAD).sum().item())
        y = torch.tensor(self.labels[i], dtype=torch.float32)
        return x, length, y

def collate(batch):
    xs, lens, ys = zip(*batch)
    X = torch.stack(xs, dim=0)
    lengths = torch.tensor(lens, dtype=torch.long)
    y = torch.stack(ys, dim=0)
    return X, lengths, y

# -----------------------
# Model
# -----------------------
class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, hidden_size: int,
                 num_layers: int = 1, dropout: float = 0.2):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD)
        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers,
                            batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size * 2, 1)  # binary
        
    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:
        # x: [B, T], lengths: [B]
        emb = self.emb(x)  # [B, T, E]
        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)
        _, (h_n, _) = self.lstm(packed)  # h_n: [num_layers*2, B, H]
        # concat last layer's forward/backward states
        if self.lstm.bidirectional:
            h = torch.cat([h_n[-2], h_n[-1]], dim=1)  # [B, 2H]
        else:
            h = h_n[-1]
        h = self.dropout(h)
        logits = self.fc(h).squeeze(1)  # [B]
        return logits  # raw (for BCEWithLogitsLoss)

# -----------------------
# Train / Eval loops
# -----------------------
def train_epoch(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0.0
    for X, lengths, y in loader:
        X, lengths, y = X.to(device), lengths.to(device), y.to(device)
        optimizer.zero_grad()
        logits = model(X, lengths)
        loss = criterion(logits, y)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        total_loss += loss.item() * X.size(0)
    return total_loss / len(loader.dataset)

@torch.no_grad()
def evaluate(model, loader, criterion):
    model.eval()
    total_loss = 0.0
    all_y, all_p = [], []
    for X, lengths, y in loader:
        X, lengths, y = X.to(device), lengths.to(device), y.to(device)
        logits = model(X, lengths)
        loss = criterion(logits, y)
        total_loss += loss.item() * X.size(0)
        probs = torch.sigmoid(logits)
        all_y.append(y.cpu().numpy())
        all_p.append(probs.cpu().numpy())
    y_true = np.concatenate(all_y)
    p = np.concatenate(all_p)
    y_pred = (p >= 0.5).astype(int)
    acc = accuracy_score(y_true, y_pred)
    p_, r_, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary", zero_division=0)
    return total_loss / len(loader.dataset), acc, f1, p_, r_

# -----------------------
# Inference helpers
# -----------------------
@torch.no_grad()
def decide_break(first_line: str, second_line: str, model: BiLSTMClassifier,
                 ch2idx: dict, max_len: int) -> int:
    s = make_window(first_line, second_line)
    x = torch.tensor(encode(s, ch2idx, max_len), dtype=torch.long).unsqueeze(0).to(device)
    length = torch.tensor([int((x != PAD).sum().item())], dtype=torch.long).to(device)
    logits = model(x, length)
    prob = torch.sigmoid(logits).item()
    return int(prob >= 0.5)

@torch.no_grad()
def reconstruct(lines: List[str], model: BiLSTMClassifier, ch2idx: dict, max_len: int) -> str:
    out = []
    for i in range(len(lines) - 1):
        a = lines[i]
        b = lines[i+1]
        join = decide_break(a, b, model, ch2idx, max_len)
        if join:
            a = a.rstrip("-")                    # drop hyphen if present
            out.append(a)
        else:
            out.append(a + " ")
    out.append(lines[-1])
    return "".join(out)

# -----------------------
# Main
# -----------------------
def main(args):
    print(f"Using device: {device}")
    df = load_data(args.data)

    # Make fixed-length window length
    MAX_LEN = left_n + 1 + right_n

    # Split
    y = df["label"].values
    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=y)
    y_temp = temp_df["label"].values
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=y_temp)

    # Build vocabulary on training set
    ch2idx, idx2ch = build_vocab(train_df["window"].tolist())
    vocab_size = len(idx2ch)

    # Datasets / Loaders
    train_ds = BoundaryDataset(train_df["window"].tolist(), train_df["label"].tolist(), ch2idx, MAX_LEN)
    val_ds   = BoundaryDataset(val_df["window"].tolist(),   val_df["label"].tolist(),   ch2idx, MAX_LEN)
    test_ds  = BoundaryDataset(test_df["window"].tolist(),  test_df["label"].tolist(),  ch2idx, MAX_LEN)

    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)
    val_dl   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate)
    test_dl  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate)

    # Model
    model = BiLSTMClassifier(vocab_size, embed_dim, hidden_size, num_layers, dropout).to(device)

    # Loss with class imbalance handling
    pos = (train_df["label"] == 1).sum()
    neg = (train_df["label"] == 0).sum()
    pos_weight = torch.tensor(neg / max(pos, 1), dtype=torch.float32).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # Training with early stopping
    best_val = math.inf
    patience_counter = patience
    best_state = None

    for epoch in range(1, epochs + 1):
        train_loss = train_epoch(model, train_dl, optimizer, criterion)
        val_loss, val_acc, val_f1, val_p, val_r = evaluate(model, val_dl, criterion)
        print(f"Epoch {epoch:02d} | train loss {train_loss:.4f} | val loss {val_loss:.4f} | val acc {val_acc:.3f} | val F1 {val_f1:.3f}")

        if val_loss < best_val - 1e-4:
            best_val = val_loss
            best_state = {
                "model": model.state_dict(),
                "ch2idx": ch2idx,
                "idx2ch": idx2ch,
                "config": {
                    "LEFT_N": left_n, "RIGHT_N": right_n,
                    "EMBED_DIM": embed_dim, "HIDDEN_SIZE": hidden_size,
                    "NUM_LAYERS": num_layers, "DROPOUT": dropout
                }
            }
            patience_counter = patience
        else:
            patience_counter -= 1
            if patience_counter == 0:
                print("Early stopping.")
                break

    # Load best and evaluate on test
    if best_state is not None:
        model.load_state_dict(best_state["model"])
    test_loss, test_acc, test_f1, test_p, test_r = evaluate(model, test_dl, criterion)
    print(f"\nTEST  | loss {test_loss:.4f} | acc {test_acc:.3f} | F1 {test_f1:.3f} | P {test_p:.3f} | R {test_r:.3f}")
    print("\nDetailed classification report on TEST:")
    # Make a one-off report
    @torch.no_grad()
    def collect_preds(dl):
        model.eval()
        y_true, y_pred = [], []
        for X, lengths, y in dl:
            logits = model(X.to(device), lengths.to(device))
            probs = torch.sigmoid(logits).cpu().numpy()
            y_true.append(y.numpy())
            y_pred.append((probs >= 0.5).astype(int))
        return np.concatenate(y_true), np.concatenate(y_pred)
    y_t, y_hat = collect_preds(test_dl)
    print(classification_report(y_t, y_hat, digits=3))

    # Save checkpoint + vocab
    torch.save(model.state_dict(), "bilstm_rejoin_model.pt")
    with open("char_vocab.json", "w", encoding="utf-8") as f:
        json.dump(best_state["ch2idx"], f, ensure_ascii=False, indent=2)

    print("\nSaved model to bilstm_rejoin_model.pt and vocab to char_vocab.json")

    example_lines = [
        "Cum in maximis periculis huius urbis atque imperi, grav",
        "issimo atque acerbissimo rei publicae casu, socio atque adiutore",
        "consiliorum periculorumque meorum L. Flacco, caedem a vobis,",
        "coniugibus, liberis vestris, vastitatem a templis, delu",
        "bris, urbe, Italia depellebam, sperabam, iudices, honoris potius L. Flac-",
        "ci me adiutorem futurum quam miseriarum deprecatorem. Quod enim esset",
        "praemium dignitatis quod populus Romanus, cum huius maioribus semper"
    ]
    text = reconstruct(example_lines, model, best_state["ch2idx"], MAX_LEN)
    print("\nReconstructed sample:")
    print(text)

if __name__ == "__main__":
      parser = argparse.ArgumentParser(description="Train a BiLSTM model for rejoining lines.")
      parser.add_argument("data", help="TSV file containing training data (columns: 'last_15', 'first_15' and 'word_break').")
      args = parser.parse_args()
      main(args)