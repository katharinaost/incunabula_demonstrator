# largely generated by ChatGPT 5

import argparse
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

left_n = 12        # chars taken from end of first line
right_n = 12       # chars taken from start of second line

def load_data(data_path):
    df = pd.read_csv(data_path, sep="\t")
    df = df.dropna(subset=['word_break']) # drop unlabeled rows
    df = df.rename(columns={"last_15": "first", "first_15": "second", "word_break": "label"})
    df["label"] = df["label"].astype(int)
    return df


def boundary_features(frame):
    left_tail = frame["first"].str[-left_n:].fillna("")
    right_head = frame["second"].str[:right_n].fillna("")
    pair = left_tail + "|" + right_head

    left_last = frame["first"].str.rstrip().str[-1:]
    ends_with_hyphen = left_last.str.endswith("-").astype(int)
    ends_with_punct = left_last.str.match(r"[.,;:!?/]").fillna(False).astype(int)

    flags = np.vstack([
        ends_with_hyphen,
        ends_with_punct
    ]).T

    return pd.DataFrame({
        "left_tail": left_tail,
        "right_head": right_head,
        "pair": pair,
        "ends_with_hyphen": ends_with_hyphen,
        "ends_with_punct": ends_with_punct
    })


def decide_break(pipe: Pipeline, first_line, second_line):
    tmp = pd.DataFrame({"first":[first_line], "second":[second_line]})
    feats = boundary_features(tmp)
    return int(pipe.predict(feats)[0])

def reconstruct(pipe: Pipeline, lines):
    out = []
    for i in range(len(lines)-1):
        a, b = lines[i], lines[i+1]
        join = decide_break(pipe, a, b)
        if join:
            a = a.rstrip("-")  # remove hyphen if present
            out.append(a)
        else:
            out.append(a + " ")
    out.append(lines[-1])
    return "".join(out)

def main(args):
    
    df = load_data(args.data)
    
    X_raw = boundary_features(df)
    
    text_cols = ["pair", "left_tail", "right_head"]
    flag_cols = ["ends_with_hyphen","ends_with_punct"]

    # vectorize text columns, pass through flag columns
    text_ct = ColumnTransformer(
        transformers=[
            ("pair", TfidfVectorizer(analyzer="char", ngram_range=(2,6), min_df=2), "pair"),
            ("left", TfidfVectorizer(analyzer="char", ngram_range=(2,6), min_df=2), "left_tail"),
            ("right", TfidfVectorizer(analyzer="char", ngram_range=(2,6), min_df=2), "right_head"),
            ("flags", "passthrough", flag_cols),
        ],
        sparse_threshold=0.3
    )

    clf = LogisticRegression(
        penalty="l2",
        solver="liblinear",
        class_weight="balanced",
        max_iter=1000
    )

    pipe = Pipeline([
        ("features", text_ct),
        ("clf", clf)
    ])

    X_train, X_test, y_train, y_test = train_test_split(X_raw, df["label"], test_size=0.1, random_state=42, stratify=df["label"])

    pipe.fit(X_train, y_train)
    pred = pipe.predict(X_test)

    print(classification_report(y_test, pred, digits=3))
    print("Confusion matrix:\n", confusion_matrix(y_test, pred))

    example_lines = [
    "Cum in maximis periculis huius urbis atque imperi, grav",
    "issimo atque acerbissimo rei publicae casu, socio atque adiutore",
    "consiliorum periculorumque meorum L. Flacco, caedem a vobis,",
    "coniugibus, liberis vestris, vastitatem a templis, delu",
    "bris, urbe, Italia depellebam, sperabam, iudices, honoris potius L. Flac-",
    "ci me adiutorem futurum quam miseriarum deprecatorem. Quod enim esset",
    "praemium dignitatis quod populus Romanus, cum huius maioribus semper"
    ]
    text = reconstruct(pipe, example_lines)
    print("\nReconstructed sample:")
    print(text)

if __name__ == "__main__":
      parser = argparse.ArgumentParser(description="Train a logistic regression model for rejoining lines.")
      parser.add_argument("data", help="TSV file containing training data (columns: 'last_15', 'first_15' and 'word_break').")
      args = parser.parse_args()
      main(args)